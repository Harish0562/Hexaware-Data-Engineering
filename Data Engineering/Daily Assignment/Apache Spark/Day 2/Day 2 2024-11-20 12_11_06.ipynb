{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbf63c0f-77b1-46b7-817d-780df15f9a7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "#initialize the program\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import  SparkSession\n",
    "\n",
    "sc =SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.appName('pyspark first program').getOrCreate()\n",
    "\n",
    "#create the rdd\n",
    "\n",
    "rdd = sc.parallelize([('C',85,76,87,91), ('B',85,76,87,91), (\"A\", 85,78,96,92), (\"A\", 92,76,89,96)])\n",
    "print(type(rdd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64ea77b4-b776-4ced-82ad-dd86d86a9ac2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\nParallelCollectionRDD[2] at readRDDFromInputStream at PythonRDD.scala:435\n+--------+-------+-----------+-------+---------+\n|Division|English|Mathematics|Physics|Chemistry|\n+--------+-------+-----------+-------+---------+\n|       C|     85|         76|     87|       91|\n|       B|     85|         76|     87|       91|\n|       A|     85|         78|     96|       92|\n|       A|     92|         76|     89|       96|\n+--------+-------+-----------+-------+---------+\n\nroot\n |-- Division: string (nullable = true)\n |-- English: long (nullable = true)\n |-- Mathematics: long (nullable = true)\n |-- Physics: long (nullable = true)\n |-- Chemistry: long (nullable = true)\n\nOut[3]: [('C', 85, 76, 87, 91),\n ('B', 85, 76, 87, 91),\n ('A', 85, 78, 96, 92),\n ('A', 92, 76, 89, 96)]"
     ]
    }
   ],
   "source": [
    "\n",
    "#to create rdds and  dataframe\n",
    "#\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import  SparkSession\n",
    "\n",
    "sc =SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.appName('pyspark first program').getOrCreate()\n",
    "\n",
    "#create the rdd\n",
    "\n",
    "rdd = sc.parallelize([('C',85,76,87,91), ('B',85,76,87,91), (\"A\", 85,78,96,92), (\"A\", 92,76,89,96)], 4)\n",
    "mydata = ['Division','English','Mathematics','Physics','Chemistry']\n",
    "marks_df = spark.createDataFrame(rdd, schema=mydata)\n",
    "print(type(marks_df))\n",
    "print(rdd)\n",
    "marks_df.show()\n",
    "marks_df.printSchema()\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc995cc9-6083-4047-8f30-16f96ca7c4d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \n",
    "                    .appName('pyspark_ex').getOrCreate()\n",
    "\n",
    "data = [('James','Smith','M',3000),\n",
    "  ('Anna','Rose','F',4100),\n",
    "  ('Robert','Williams','M',6200), \n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b68c850-5ecb-45ad-9237-dc2328e5c4ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#to create rdds and  dataframe\n",
    "#\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import  SparkSession\n",
    "\n",
    "sc =SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.appName('pyspark first program').getOrCreate()\n",
    "\n",
    "data =spark.read.csv(\"/FileStore/tables/orders.csv\",header = True,inferSchema = True)\n",
    "data.show()\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3673d26e-478b-4f20-b62f-7c235405f33f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#to create rdds and  dataframe\n",
    "#\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import  SparkSession\n",
    "\n",
    "sc =SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.appName('pyspark first program').getOrCreate()\n",
    "\n",
    "data =spark.read.csv(\"/FileStore/tables/orders.csv\")\n",
    "data.show()\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "379eff6d-272f-4265-bf6f-ceb031fff5dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName('pyspark_ex').getOrCreate()\n",
    "\n",
    "data = [('James','Smith','M',3000),\n",
    "  ('Anna','Rose','F',4100),\n",
    "  ('Robert','Williams','M',6200), \n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n",
    "df.show()\n",
    "\n",
    "#1.write a program for adding a new column\n",
    "from pyspark.sql.functions import lit\n",
    "df.withColumn(\"new_column\",lit(1)).show()\n",
    "df.withColumn(\"other_column\",df.salary*10).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e8bf31b-f9a5-485f-9523-9a12920e95b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#to create rdds and  dataframe\n",
    "#\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import  SparkSession\n",
    "\n",
    "sc =SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.appName('pyspark first program').getOrCreate()\n",
    "\n",
    "data =spark.read.csv(\"/FileStore/tables/salary-2.csv\",header = True,inferSchema = True)\n",
    "\n",
    "data.limit(10).toPandas\n",
    "data.show()\n",
    "display(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ad344d8-e21a-42f5-bf97-d7bfdb5a9b91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Name  Age\n0   Scott   50\n1    Jeff   45\n2  Thomas   54\n3     Ann   34\n+------+---+\n|  Name|Age|\n+------+---+\n| Scott| 50|\n|  Jeff| 45|\n|Thomas| 54|\n|   Ann| 34|\n+------+---+\n\nroot\n |-- Name: string (nullable = true)\n |-- Age: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#Converting Pandasdf to spark df\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import  SparkSession\n",
    "\n",
    "sc =SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.appName('pyspark first program').getOrCreate()\n",
    "import pandas as pd    \n",
    "data = [['Scott', 50], ['Jeff', 45], ['Thomas', 54],['Ann',34]] \n",
    "  \n",
    "# Create the pandas DataFrame \n",
    "pandasDF = pd.DataFrame(data, columns = ['Name', 'Age']) \n",
    "  \n",
    "# print dataframe. \n",
    "print(pandasDF)\n",
    "\n",
    "sparkdf =spark.createDataFrame(pandasDF)\n",
    "sparkdf.show()\n",
    "sparkdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ed8d734-dba6-49ea-8808-cb2aa4c13ec9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- First Name: string (nullable = true)\n |-- Age: integer (nullable = true)\n\n+----------+---+\n|First Name|Age|\n+----------+---+\n|     Scott| 50|\n|      Jeff| 45|\n|    Thomas| 54|\n|       Ann| 34|\n+----------+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "mySchema = StructType([ StructField(\"First Name\", StringType(), True)\\\n",
    "                       ,StructField(\"Age\", IntegerType(), True)])\n",
    "\n",
    "sparkDF2 = spark.createDataFrame(pandasDF,schema=mySchema)\n",
    "sparkDF2.printSchema()\n",
    "sparkDF2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d5f96f8-aa7c-44f3-afa7-d942d3ee98e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method PandasConversionMixin.toPandas of DataFrame[First Name: string, Age: int]>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\",\"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.fallback.enabled\",\"true\")\n",
    "\n",
    "pandasDF2=sparkDF2.select(\"*\").toPandas\n",
    "print(pandasDF2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98f86d4b-42f3-4ff9-ae05-4232e79cb098",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+\n|      date|increment|  inc_date|\n+----------+---------+----------+\n|2019-01-23|        1|2019-02-23|\n|2019-06-24|        2|2019-08-24|\n|2019-09-20|        3|2019-12-20|\n+----------+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('pyspark_ex4').getOrCreate()\n",
    "\n",
    "from pyspark.sql.functions import col,expr\n",
    "\n",
    "data=[(\"2019-01-23\",1),(\"2019-06-24\",2),(\"2019-09-20\",3)]\n",
    "spark.createDataFrame(data).toDF(\"date\",\"increment\") \\\n",
    "    .select(col(\"date\"),col(\"increment\"), \\\n",
    "      expr(\"add_months(to_date(date,'yyyy-MM-dd'),cast(increment as int))\").alias(\"inc_date\")) \\\n",
    "    .show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Day 2 2024-11-20 12:11:06",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
