{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5903e2b4-6f9f-47a0-a5a7-df71775b8e94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3])\n",
    "print(rdd.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d9bf0b0-f794-4b79-abe1-39b784760151",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "rdd = sc.parallelize([1, 2, 3, 4])\n",
    "print(rdd.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f4f3ffa-1cd6-4eb4-bd2f-eca9d4da3f1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "rdd = sc.parallelize([10, 20, 30])\n",
    "print(rdd.first())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e628b2c6-adf7-47b4-8e25-256a7f00d343",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rdd = sc.parallelize([5, 6, 7, 8])\n",
    "print(rdd.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "189012d9-ca16-40ba-a31e-b96883b74da5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "rdd = sc.parallelize([1, 2, 3])\n",
    "print(rdd.reduce(lambda x, y: x + y))  # Sum of elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43abb2da-1b16-4b61-9993-01aa8c3e966b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "rdd = sc.parallelize([10, 20, 30])\n",
    "rdd.saveAsTextFile('files.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27888c7c-dc66-4163-a296-78c012cf68ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3])\n",
    "print(rdd.map(lambda x: x * 2).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64121895-e866-4070-ba32-9ccbc90096cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4])\n",
    "print(rdd.filter(lambda x: x % 2 == 0).collect())  # Even numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65f6c3ea-f693-4af1-8b5a-95130aa326af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 6, 9]\n"
     ]
    }
   ],
   "source": [
    "union_inp = sc.parallelize([2,4,5,6,7,8,9])\n",
    "union_rdd_1 = union_inp.filter(lambda x: x % 2 == 0)\n",
    "union_rdd_2 = union_inp.filter(lambda x: x % 3 == 0)\n",
    "print(union_rdd_1.union(union_rdd_2).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f03c81cc-4be3-48c2-a2d3-b13f7c240485",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'PySpark', 'RDD']\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([\"hello world\", \"PySpark RDD\"])\n",
    "print(rdd.flatMap(lambda x: x.split(\" \")).collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8c96241-b37b-45e6-9186-98cbcb03fd8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Shreya', 50), ('Swati', 45), ('Rahul', 48), ('Abhay', 55), ('Rohan', 44)]\n"
     ]
    }
   ],
   "source": [
    "marks_rdd = sc.parallelize([('Rahul', 25), ('Swati', 26), ('Shreya', 22), ('Abhay', 29), ('Rohan', 22),\n",
    "('Rahul', 23), ('Swati', 19), ('Shreya', 28), ('Abhay', 26), ('Rohan', 22)])\n",
    "print(marks_rdd.reduceByKey(lambda x, y: x + y).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29007408-859b-4d49-8ee5-71f4dc7e8aed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Abhay', 29), ('Abhay', 26), ('Rahul', 25), ('Rahul', 23), ('Rohan', 22), ('Rohan', 22), ('Shreya', 22), ('Shreya', 28), ('Swati', 26), ('Swati', 19)]\n"
     ]
    }
   ],
   "source": [
    "marks_rdd = sc.parallelize([('Rahul', 25), ('Swati', 26), ('Shreya', 22), ('Abhay', 29), ('Rohan', 22),\n",
    "('Rahul', 23), ('Swati', 19), ('Shreya', 28), ('Abhay', 26), ('Rohan', 22)])\n",
    "print(marks_rdd.sortByKey('ascending').collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4217fb92-1282-43ee-85c0-0db6902248b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shreya [22, 28]\nSwati [26, 19]\nRahul [25, 23]\nAbhay [29, 26]\nRohan [22, 22]\n"
     ]
    }
   ],
   "source": [
    "marks_rdd = sc.parallelize([('Rahul', 25), ('Swati', 26), ('Shreya', 22), ('Abhay', 29), ('Rohan', 22),\n",
    "('Rahul', 23), ('Swati', 19), ('Shreya', 28), ('Abhay', 26), ('Rohan', 22)])\n",
    "dict_rdd = marks_rdd.groupByKey().collect()\n",
    "for key, value in dict_rdd:\n",
    "  print(key, list(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6d99830-6791-4431-a1bb-9ceb653ac2c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rahul 2\nSwati 2\nRohan 2\nShreya 1\nAbhay 1\n"
     ]
    }
   ],
   "source": [
    "marks_rdd = sc.parallelize([('Rahul', 25), ('Swati', 26), ('Rohan', 22), ('Rahul', 23), ('Swati', 19),\n",
    "('Shreya', 28), ('Abhay', 26), ('Rohan', 22)])\n",
    "dict_rdd = marks_rdd.countByKey().items()\n",
    "for key, value in dict_rdd:\n",
    "  print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c4fb0d5-4da7-412c-b91b-dce5b5aef967",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n| Name|\n+-----+\n| Ravi|\n|Meena|\n| Arun|\n+-----+\n\n+-----+---+\n| Name|Age|\n+-----+---+\n| Ravi| 25|\n|Meena| 30|\n| Arun| 22|\n+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"PySparkExample\").getOrCreate()\n",
    "\n",
    "data = [('Ravi', 25, 'Delhi'), ('Meena', 30, 'Mumbai'), ('Arun', 22, 'Chennai')]\n",
    "columns = ['Name', 'Age', 'City']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Select single column\n",
    "df.select('Name').show()\n",
    "\n",
    "# Select multiple columns\n",
    "df.select('Name', 'Age').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f21c9297-a19e-4a90-b3ba-bffbe54731d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+\n| Name|Years|   City|\n+-----+-----+-------+\n| Ravi|   25|  Delhi|\n|Meena|   30| Mumbai|\n| Arun|   22|Chennai|\n+-----+-----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# Rename a column\n",
    "renamed_df = df.withColumnRenamed('Age', 'Years')\n",
    "renamed_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e50531bd-241d-4366-840a-1f5e1de74f94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+\n| Name|Age|  City|\n+-----+---+------+\n|Meena| 30|Mumbai|\n+-----+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where Age > 25\n",
    "filtered_df = df.filter(df['Age'] > 25)\n",
    "filtered_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20ecf563-dd3c-4f89-8826-b45a8ee4ee5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------+-----------+\n| Name|Age|   City|AgeNextYear|\n+-----+---+-------+-----------+\n| Ravi| 25|  Delhi|         26|\n|Meena| 30| Mumbai|         31|\n| Arun| 22|Chennai|         23|\n+-----+---+-------+-----------+\n\n+-----+---+\n| Name|Age|\n+-----+---+\n| Ravi| 25|\n|Meena| 30|\n| Arun| 22|\n+-----+---+\n\n+-----+---+-------+\n| Name|Age|   City|\n+-----+---+-------+\n|Meena| 30| Mumbai|\n| Ravi| 25|  Delhi|\n| Arun| 22|Chennai|\n+-----+---+-------+\n\n+-------+--------+\n|   City|avg(Age)|\n+-------+--------+\n|  Delhi|    25.0|\n| Mumbai|    30.0|\n|Chennai|    22.0|\n+-------+--------+\n\n+-------+-----+---+------+\n|   City| Name|Age|Region|\n+-------+-----+---+------+\n|Chennai| Arun| 22| South|\n|  Delhi| Ravi| 25| North|\n| Mumbai|Meena| 30|  West|\n+-------+-----+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Add a new column with calculated values\n",
    "df_with_new_col = df.withColumn('AgeNextYear', df['Age'] + 1)\n",
    "df_with_new_col.show()\n",
    "# Drop a column\n",
    "df_dropped = df.drop('City')\n",
    "df_dropped.show()\n",
    "# Sort by Age in descending order\n",
    "df_sorted = df.orderBy(df['Age'].desc())\n",
    "df_sorted.show()\n",
    "# Group by City and calculate the average age\n",
    "df.groupBy('City').avg('Age').show()\n",
    "# Joining two DataFrames\n",
    "data2 = [('Delhi', 'North'), ('Mumbai', 'West'), ('Chennai', 'South')]\n",
    "columns2 = ['City', 'Region']\n",
    "\n",
    "df2 = spark.createDataFrame(data2, columns2)\n",
    "\n",
    "joined_df = df.join(df2, on='City', how='inner')\n",
    "joined_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89bd379a-1d59-430b-ae8e-2426c9c82828",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------+--------------+\n| Name|Age|   City|NameWithPrefix|\n+-----+---+-------+--------------+\n| Ravi| 25|  Delhi|  Mr./Ms. Ravi|\n|Meena| 30| Mumbai| Mr./Ms. Meena|\n| Arun| 22|Chennai|  Mr./Ms. Arun|\n+-----+---+-------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Define a UDF to add \"Mr./Ms.\" prefix\n",
    "def add_prefix(name):\n",
    "    return f\"Mr./Ms. {name}\"\n",
    "\n",
    "add_prefix_udf = udf(add_prefix, StringType())\n",
    "\n",
    "# Apply UDF\n",
    "df_with_prefix = df.withColumn('NameWithPrefix', add_prefix_udf(df['Name']))\n",
    "df_with_prefix.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f54bf0b-0b63-46c4-9b41-622fcc37d171",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+\n| Name|Age|  City|\n+-----+---+------+\n|Meena| 30|Mumbai|\n+-----+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"PySparkExample\").getOrCreate()\n",
    "\n",
    "data = [('Ravi', 25, 'Delhi'), ('Meena', 30, 'Mumbai'), ('Arun', 22, 'Chennai')]\n",
    "columns = ['Name', 'Age', 'City']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Create a temporary view\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# Query the view using SQL\n",
    "spark.sql(\"SELECT * FROM people WHERE Age > 25\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63f6bf20-d683-441d-8d1d-a3dfd432c6f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------+\n| Name|Age|   City|\n+-----+---+-------+\n| Ravi| 25|  Delhi|\n|Meena| 30| Mumbai|\n| Arun| 22|Chennai|\n+-----+---+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# Create a global temporary view\n",
    "df.createGlobalTempView(\"global_people\")\n",
    "\n",
    "# Query the global view using SQL (prefix with `global_temp.`)\n",
    "spark.sql(\"SELECT * FROM global_temp.global_people\").show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Untitled Notebook 2024-11-19 11:02:41",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
